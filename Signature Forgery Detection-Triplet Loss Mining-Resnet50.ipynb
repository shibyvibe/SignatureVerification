{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color: red;\">TODO </b>\n",
    "1) Add signatures from dataset1 to dataset2 mainly related to background color <br>\n",
    "2) Understand how to setup tensorflow GPU. Currently using Tensorflow CPU\n",
    "\n",
    "<p style=\"color: red;\">\n",
    "Tensor flow install error\n",
    "\n",
    "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
    "Successfully built wrapt\n",
    "ERROR: catboost 0.24.4 requires plotly, which is not installed.\n",
    "ERROR: tensorflow 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\n",
    "ERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.4.1 which is incompatible.\n",
    "ERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\n",
    "ERROR: tensorflow-gpu 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\n",
    "ERROR: tensorflow-gpu 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.4.1 which is incompatible.\n",
    "ERROR: tensorflow-gpu 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\n",
    "ERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\n",
    "Installing collected packages: six, absl-py, gast, grpcio, numpy, oauthlib, requests-oauthlib, rsa, cachetools, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, wheel, tensorboard, wrapt, typing-extensions, keras-preprocessing, astunparse, opt-einsum, tensorflow-estimator, flatbuffers, tensorflow-cpu\n",
    "  Attempting uninstall: six\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tuning </b><p>\n",
    "layer.trainable = True yields around ~77% accuracy\n",
    "\n",
    "To try\n",
    "1) Increase input shape to higher than 224   ==> imageDimensions = (224,224,3)=77%   (512,224,3)= <br> \n",
    "2) Add one more FC Layer ===> 4096 did not help <br>\n",
    "3) Try Resnet<br>\n",
    "\n",
    "VGG16 and VGG19 - yields similar performance.\n",
    "    \n",
    "</p>\n",
    "\n",
    "Trying retraining all layers instead of using exsisting weights. 78% accuracy!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imageDimensions = (224,224,3)\n",
    "imageDimensions = (224,224,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Utility functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(filename):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPEG image, preprocess it and\n",
    "    resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_png(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, imageDimensions[:-1])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplets_train( anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "    print(\">>> Anchor\", anchor)\n",
    "\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplets(personId, anchor, positive, negative, isGenuine):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "    print(\">>> Anchor\", anchor)\n",
    "\n",
    "    return (\n",
    "        personId,\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "        isGenuine\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def triplet_loss(y_true, y_pred):\n",
    "#     alpha = 0.5\n",
    "#     anchor, positive, negative =y_pred[0,0:512], y_pred[0,512:1024], y_pred[0,1024:1536]\n",
    "    \n",
    "#     positive_distance = K.mean(K.square(anchor - positive),axis=-1)\n",
    "#     negative_distance = K.mean(K.square(anchor - negative),axis=-1)\n",
    "#     return K.mean(K.maximum(0.0, positive_distance - negative_distance + alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getImage(basePath, typeOfData, dfRec, imageDimensions):\n",
    "#     img_path = basePath + \"/\" + typeOfData + \"/\" + dfRec[\"relPath\"] + \"/\" + dfRec[\"fileName\"]\n",
    "#     img = cv2.imread(img_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = cv2.resize(img, (imageDimensions[1],imageDimensions[0]), interpolation=cv2.INTER_CUBIC)\n",
    "#     #img = img/255 - resulted in a drop in accuracy\n",
    "#     #preprocess_input(img)\n",
    "#     return img;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(images_dataset):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def showImages(ax, image):\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(image[i])\n",
    "            ax[i].get_xaxis().set_visible(False)\n",
    "            ax[i].get_yaxis().set_visible(False)\n",
    "\n",
    "    rows = len(images_dataset)\n",
    "    images = list(images_dataset.as_numpy_iterator())\n",
    "    fig, axs = plt.subplots(rows,3, sharex=True, sharey=True, figsize=(500,500))\n",
    "    for x in range(rows):\n",
    "         anchor, positive, negative = images[x][0],images[x][1],images[x][2]\n",
    "         showImages(axs[x], (anchor, positive, negative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataGenerator(df, typeOfData):\n",
    "#     personIds = df[\"personId\"].unique()\n",
    "#     for p in personIds:\n",
    "#         genuine = df[(df.personId==p) & (df.Genuine==1)]\n",
    "#         forg = df[(df.personId==p) & (df.Genuine==0)]\n",
    "#         anchor_img = getImage(basePath, typeOfData, genuine.iloc[0], imageDimensions) # Use first row as anchor\n",
    "#         for g in genuine[1:].index:\n",
    "#             pos_img= getImage(basePath, typeOfData, genuine.loc[g], imageDimensions) \n",
    "#             for f in forg.index:\n",
    "#                 neg_img =  getImage(basePath, typeOfData, forg.loc[f], imageDimensions)\n",
    "#                 #yield(np.array([anchor_img, pos_img, neg_img]), np.array([0]))\n",
    "#                 print (type(anchor_img),type(pos_img), type(neg_img), np.array([0]))\n",
    "#                 print (anchor_img.shape,pos_img.shape, neg_img.shape, np.array([0]))\n",
    "#                 #yield([np.array(anchor_img), np.array(pos_img), np.array(neg_img)], np.array([0]))\n",
    "#                 #yield([np.array(anchor_img), np.array(pos_img), np.array(neg_img)],np.array([0]))\n",
    "#                 yield([anchor_img, pos_img, neg_img], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load training and test data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = \"/notebooks/capstone/dataset/dataset2/sign_data\"\n",
    "data_train = pd.read_csv(basePath + \"/train/train_clean.csv\")\n",
    "data_train.sort_values(by=\"personId\", inplace=True)\n",
    "print(data_train.head())\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(basePath + \"/test/test_clean.csv\")\n",
    "data_test.sort_values(by=\"personId\", inplace=True)\n",
    "print(data_test.head(20))\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare training data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeImages(df, typeOfData):\n",
    "    personIds = df[\"personId\"].unique()\n",
    "    anchor_imgs = []\n",
    "    postive_imgs = []\n",
    "    negative_imgs = []\n",
    "    for p in personIds:\n",
    "        genuine = df[(df.personId==p) & (df.Genuine==1)]\n",
    "        forg = df[(df.personId==p) & (df.Genuine==0)]\n",
    "        anchor_img = basePath+\"/\"+typeOfData+\"/\"+genuine.iloc[0].relPath + \"/\"+genuine.iloc[0].fileName  \n",
    "        for g in genuine[1:].index:\n",
    "            pos_img= basePath+\"/\"+typeOfData+\"/\"+genuine.loc[g].relPath + \"/\"+genuine.loc[g].fileName  \n",
    "            for f in forg.index:\n",
    "                neg_img =  basePath+\"/\"+typeOfData+\"/\"+forg.loc[f].relPath + \"/\"+forg.loc[f].fileName  \n",
    "                anchor_imgs.append(anchor_img)\n",
    "                postive_imgs.append(pos_img)\n",
    "                negative_imgs.append(neg_img)\n",
    "    \n",
    "    return anchor_imgs,postive_imgs,negative_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anchor_dataset,positive_dataset,negative_dataset = tf.data.Dataset.from_generator(generator=categorizeImages(data_train, \"train\"))\n",
    "anchor_images,positive_images,negative_images = categorizeImages(data_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "positive_dataset  = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "negative_dataset  = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.map(preprocess_triplets_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split our dataset in train and validation.\n",
    "image_count = len(anchor_dataset)\n",
    "train_dataset = dataset.take(round(image_count * 0.8))\n",
    "val_dataset = dataset.skip(round(image_count * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize(train_dataset.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare Test data set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare each positive each positive in the set and and each negative against all other positives\n",
    "def categorizeTestImages(df, typeOfData):\n",
    "    personIds = df[\"personId\"].unique()\n",
    "    anchor_imgs = []\n",
    "    postive_imgs = []\n",
    "    toCompare_imgs = []\n",
    "    isGenuine = []\n",
    "    personId = []\n",
    "    for p in personIds:\n",
    "        genuine = df[(df.personId==p) & (df.Genuine==1)]\n",
    "        forg = df[(df.personId==p) & (df.Genuine==0)]\n",
    "        anchor_img = basePath+\"/\"+typeOfData+\"/\"+genuine.iloc[0].relPath + \"/\"+genuine.iloc[0].fileName  \n",
    "        for g in genuine[1:].index:\n",
    "            pos_img= basePath+\"/\"+typeOfData+\"/\"+genuine.loc[g].relPath + \"/\"+genuine.loc[g].fileName  \n",
    "            #Compare this with all forgeries\n",
    "            for f in forg.index:\n",
    "                toCompareImg =  basePath+\"/\"+typeOfData+\"/\"+forg.loc[f].relPath + \"/\"+forg.loc[f].fileName \n",
    "                personId.append(p)\n",
    "                anchor_imgs.append(anchor_img)\n",
    "                postive_imgs.append(pos_img)\n",
    "                toCompare_imgs.append(toCompareImg)\n",
    "                isGenuine.append(False)\n",
    "                \n",
    "            # Compare current postive with all other positives besides the anchor\n",
    "            for f in genuine[1:].index:\n",
    "                toCompareImg =  basePath+\"/\"+typeOfData+\"/\"+genuine.loc[f].relPath + \"/\"+genuine.loc[f].fileName  \n",
    "                if ( pos_img != toCompareImg):\n",
    "                    personId.append(p)\n",
    "                    anchor_imgs.append(anchor_img)\n",
    "                    postive_imgs.append(pos_img)\n",
    "                    toCompare_imgs.append(toCompareImg)\n",
    "                    isGenuine.append(True)    \n",
    "    return personId, anchor_imgs,postive_imgs,toCompare_imgs, isGenuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_personId, tst_anchor_images,tst_positive_images,tst_toCompare_imgs, tst_isGenuine = categorizeTestImages(data_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tst_personId))\n",
    "print(len(tst_anchor_images))\n",
    "print(len(tst_positive_images))\n",
    "print(len(tst_toCompare_imgs))\n",
    "print(len(tst_isGenuine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_personId_dataset = tf.data.Dataset.from_tensor_slices(tst_personId)\n",
    "tst_anchor_dataset = tf.data.Dataset.from_tensor_slices(tst_anchor_images)\n",
    "tst_positive_dataset  = tf.data.Dataset.from_tensor_slices(tst_positive_images)\n",
    "tst_toCompare_imgs_dataset  = tf.data.Dataset.from_tensor_slices(tst_toCompare_imgs)\n",
    "tst_isGenuine_dataset  = tf.data.Dataset.from_tensor_slices(tst_isGenuine)\n",
    "\n",
    "tst_dataset = tf.data.Dataset.zip((tst_personId_dataset, tst_anchor_dataset, tst_positive_dataset, tst_toCompare_imgs_dataset, tst_isGenuine_dataset ))\n",
    "##dataset = dataset.shuffle(buffer_size=1024)\n",
    "tst_dataset = tst_dataset.map(preprocess_triplets)\n",
    "len(tst_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize(tst_dataset.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare Model Architecture</b>\n",
    "Setting up the embedding generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=imageDimensions, include_top=False\n",
    ")\n",
    "\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "#     if layer.name == \"conv5_block1_out\":     ##TODO: Why only this layer?\n",
    "#         trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Setting up the Siamese Network model</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=imageDimensions)\n",
    "positive_input = layers.Input(name=\"positive\", shape=imageDimensions)\n",
    "negative_input = layers.Input(name=\"negative\", shape=imageDimensions)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(resnet.preprocess_input(anchor_input)),   ##TODO : What is pre-process input do here?\n",
    "    embedding(resnet.preprocess_input(positive_input)),\n",
    "    embedding(resnet.preprocess_input(negative_input)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances  ##TODO: Not clear what Output does here?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train Model </b>\n",
    "<p>We now need to implement a model with custom training loop so we can compute the triplet loss using the three embeddings produced by the Siamese network.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "         \n",
    "    def _compute_loss_sq(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    \n",
    "#     def _compute_loss_cos(self, data):\n",
    "#         cosine_similarity = metrics.CosineSimilarity()\n",
    "#         sitive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "#         print(\"Positive similarity:\", positive_similarity)\n",
    "#         negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "#         print(\"Negative similarity\", negative_similarity)\n",
    "#         loss = tf.math.squared_difference(positive_similarity, negative_similarity)\n",
    "#         loss = tf.maximum(loss + self.margin, 0.0)\n",
    "#         #tf.math.squared_difference\n",
    "#         return loss\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        return self._compute_loss_sq(data)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1)\n",
    "\n",
    "early_stop=[earlyStopping]\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0000001))\n",
    "siamese_model.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=early_stop)\n",
    "\n",
    "#Tuning\n",
    "# Image dimension=448, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=2.1\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=0.05\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, lossFn=cos, loss=? \n",
    "# Image dimension=224, margin=10 epoch 1, norestraining of weights,Adam=0.0001, loss=0.5\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights, Adam=0.00001 instead of 0.0001 loss=?**  0.000001 - 0.0020 - val_loss: 0.0095\n",
    "# Image dimension=112, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/notebooks/capstone/models/embeddings-res32'\n",
    "filename2 = '/notebooks/capstone/models/siamesenetwork-res32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding.save(filename)\n",
    "# siamese_network.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load model\n",
    "embedding = tf.keras.models.load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network = tf.keras.models.load_model(filename2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Testing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting what the network has learned\n",
    "At this point, we can check how the network learned to separate the embeddings depending on whether they belong to similar images.\n",
    "\n",
    "We can use cosine similarity to measure the similarity between embeddings.\n",
    "\n",
    "Let's pick a sample from the dataset to check the similarity between the embeddings generated for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the cosine similarity between the anchor and positive images and compare it with the similarity between the anchor and the negative images.\n",
    "\n",
    "We should expect the similarity between the anchor and positive images to be larger than the similarity between the anchor and the negative images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = tst_dataset.batch(1, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(tst_dataset))\n",
    "personId, anchor, positive, toCompare, isGenuine = sample\n",
    "print(f\"PersonId: %s anchor: %d positive: %d toCompare: %d isGenuine %s\" % (personId.numpy(), len(anchor), len(positive), len(toCompare),isGenuine.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding, positive_embedding, negative_embedding = (\n",
    "    embedding(resnet.preprocess_input(anchor)),\n",
    "    embedding(resnet.preprocess_input(positive)),\n",
    "    embedding(resnet.preprocess_input(toCompare)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PersonId: %d anchor: %d positive: %d toCompare: %d isGenuine %d\" % (len(personId), len(anchor), len(positive), len(toCompare),len(isGenuine)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the cosine similarity between the anchor and positive images and compare it with the similarity between the anchor and the negative images.\n",
    "\n",
    "We should expect the similarity between the anchor and positive images to be larger than the similarity between the anchor and the negative images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareEmbeds(personId, anchor_embedding, positive_embedding, negative_embedding, isGenuine):\n",
    "    cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "    positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "    #print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "    negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "    #print(\"Negative similarity\", negative_similarity.numpy())\n",
    "    return (personId.numpy(), positive_similarity.numpy(), negative_similarity.numpy(), (positive_similarity.numpy() - negative_similarity.numpy()), isGenuine.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (compareEmbeds(personId,anchor_embedding, positive_embedding, negative_embedding, isGenuine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(row):\n",
    "    personId, anchor, positive, toCcompare, isGenuine = row\n",
    "    return (\n",
    "        personId.numpy(), \n",
    "        embedding(resnet.preprocess_input(anchor)),\n",
    "        embedding(resnet.preprocess_input(positive)),\n",
    "        embedding(resnet.preprocess_input(toCcompare)),\n",
    "        isGenuine.numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the embeddings\n",
    "tst_embedding_data = [getEmbeddings(row) for row in iter(tst_dataset)]\n",
    "tst_data_embeddings_df = pd.DataFrame(columns=[\"personId\", \"anchor_embedding\", \"positive_embedding\",\"negative_embedding\",\"isGenuine\"], data=tst_embedding_data)\n",
    "tst_data_embeddings_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pFile_embeddings = \"/notebooks/capstone/results_embeddings.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pFile_embeddings, 'wb') as file:\n",
    "#     pickle.dump(tst_data_embeddings_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5038, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pFile_embeddings, 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['personId', 'anchor_embedding', 'positive_embedding',\n",
       "       'negative_embedding', 'isGenuine'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Approach 1</b>: Compare the relative distance of positive and negative related to Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareEmbeds(personId, anchor_embedding, positive_embedding, negative_embedding, isGenuine):\n",
    "    cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "    positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "    #print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "    negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "    #print(\"Negative similarity\", negative_similarity.numpy())\n",
    "    return (positive_similarity.numpy(), negative_similarity.numpy(), (positive_similarity.numpy() - negative_similarity.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970792</td>\n",
       "      <td>0.021536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.972754</td>\n",
       "      <td>0.019574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970110</td>\n",
       "      <td>0.022218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>0.996557</td>\n",
       "      <td>0.994285</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>0.996557</td>\n",
       "      <td>0.997044</td>\n",
       "      <td>-0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>0.996557</td>\n",
       "      <td>0.996651</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>0.996557</td>\n",
       "      <td>0.996309</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>0.996557</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>-0.000392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5038 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0     0.992328  0.970792  0.021536\n",
       "1     0.992328  0.990692  0.001636\n",
       "2     0.992328  0.978380  0.013948\n",
       "3     0.992328  0.972754  0.019574\n",
       "4     0.992328  0.970110  0.022218\n",
       "...        ...       ...       ...\n",
       "5033  0.996557  0.994285  0.002272\n",
       "5034  0.996557  0.997044 -0.000487\n",
       "5035  0.996557  0.996651 -0.000094\n",
       "5036  0.996557  0.996309  0.000248\n",
       "5037  0.996557  0.996949 -0.000392\n",
       "\n",
       "[5038 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.apply(lambda row: compareEmbeds(row.personId, row.anchor_embedding, row.positive_embedding, row.negative_embedding, row.isGenuine), axis=1,  result_type='expand')\n",
    "df2.columns = [\"Pos\", \"Neg\", \"Diff\"]\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.8645718, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970792</td>\n",
       "      <td>0.021536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.39470863, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.36604935, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "1        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "2        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "1  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "2  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "\n",
       "                                  negative_embedding  isGenuine       Pos  \\\n",
       "0  ((tf.Tensor(0.8645718, shape=(), dtype=float32...      False  0.992328   \n",
       "1  ((tf.Tensor(0.39470863, shape=(), dtype=float3...      False  0.992328   \n",
       "2  ((tf.Tensor(0.36604935, shape=(), dtype=float3...      False  0.992328   \n",
       "\n",
       "        Neg      Diff  \n",
       "0  0.970792  0.021536  \n",
       "1  0.990692  0.001636  \n",
       "2  0.978380  0.013948  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Pos\", \"Neg\", \"Diff\"]]=df2[[\"Pos\", \"Neg\", \"Diff\"]]\n",
    "df[\"personId\"]=df[\"personId\"].apply(lambda x: x[0])\n",
    "df[\"isGenuine\"]=df[\"isGenuine\"].apply(lambda x: x[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pFile = \"/notebooks/capstone/results_approach1.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pFile, 'wb') as file:\n",
    "#      pickle.dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.8645718, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970792</td>\n",
       "      <td>0.021536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.39470863, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.36604935, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.72039074, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.972754</td>\n",
       "      <td>0.019574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.6860671, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970110</td>\n",
       "      <td>0.022218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "1        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "2        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "3        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "4        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "1  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "2  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "3  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "4  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "\n",
       "                                  negative_embedding  isGenuine       Pos  \\\n",
       "0  ((tf.Tensor(0.8645718, shape=(), dtype=float32...      False  0.992328   \n",
       "1  ((tf.Tensor(0.39470863, shape=(), dtype=float3...      False  0.992328   \n",
       "2  ((tf.Tensor(0.36604935, shape=(), dtype=float3...      False  0.992328   \n",
       "3  ((tf.Tensor(0.72039074, shape=(), dtype=float3...      False  0.992328   \n",
       "4  ((tf.Tensor(0.6860671, shape=(), dtype=float32...      False  0.992328   \n",
       "\n",
       "        Neg      Diff  \n",
       "0  0.970792  0.021536  \n",
       "1  0.990692  0.001636  \n",
       "2  0.978380  0.013948  \n",
       "3  0.972754  0.019574  \n",
       "4  0.970110  0.022218  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pFile, 'rb') as file:\n",
    "     df = pickle.load(file)\n",
    "\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.8645718, shape=(), dtype=float32...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970792</td>\n",
       "      <td>0.021536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.39470863, shape=(), dtype=float3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.990692</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.36604935, shape=(), dtype=float3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.72039074, shape=(), dtype=float3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.972754</td>\n",
       "      <td>0.019574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.48840958, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.5204592, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.6860671, shape=(), dtype=float32...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992328</td>\n",
       "      <td>0.970110</td>\n",
       "      <td>0.022218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "1        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "2        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "3        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "4        49  ((tf.Tensor(0.48840958, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "1  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "2  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "3  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "4  ((tf.Tensor(0.5204592, shape=(), dtype=float32...   \n",
       "\n",
       "                                  negative_embedding  isGenuine       Pos  \\\n",
       "0  ((tf.Tensor(0.8645718, shape=(), dtype=float32...          0  0.992328   \n",
       "1  ((tf.Tensor(0.39470863, shape=(), dtype=float3...          0  0.992328   \n",
       "2  ((tf.Tensor(0.36604935, shape=(), dtype=float3...          0  0.992328   \n",
       "3  ((tf.Tensor(0.72039074, shape=(), dtype=float3...          0  0.992328   \n",
       "4  ((tf.Tensor(0.6860671, shape=(), dtype=float32...          0  0.992328   \n",
       "\n",
       "        Neg      Diff  \n",
       "0  0.970792  0.021536  \n",
       "1  0.990692  0.001636  \n",
       "2  0.978380  0.013948  \n",
       "3  0.972754  0.019574  \n",
       "4  0.970110  0.022218  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isGenuine\"] = df[\"isGenuine\"].map(lambda x: 1 if x else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/springboard/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diff</th>\n",
       "      <th>isGenuine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.215364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.195741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Diff  isGenuine\n",
       "0  0.215364          0\n",
       "1  0.016360          0\n",
       "2  0.139477          0\n",
       "3  0.195741          0\n",
       "4  0.222183          0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df[[\"Diff\",\"isGenuine\"]]\n",
    "df2[\"Diff\"] = df2[\"Diff\"]*10\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_f = df2[(df2.isGenuine == 0)]  # & (df2.Diff < threshold)\n",
    "df3_g = df2[(df2.isGenuine == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diff</th>\n",
       "      <th>isGenuine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003446</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.027829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Diff  isGenuine\n",
       "12  0.006700          1\n",
       "13  0.008011          1\n",
       "14  0.003772          1\n",
       "15  0.003446          1\n",
       "16  0.027829          1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,  22.,  86., 250., 790., 790., 250.,  86.,  22.,   7.]),\n",
       " array([-0.06793439, -0.05434752, -0.04076064, -0.02717376, -0.01358688,\n",
       "         0.        ,  0.01358688,  0.02717376,  0.04076064,  0.05434752,\n",
       "         0.06793439]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 0.01\n",
    "\n",
    "df4_f = df3_f.copy(deep=True)\n",
    "df4_f[\"Diff\"] = df4_f[\"Diff\"].map(lambda x: x if  x < threshold else threshold)\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Forgery')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Total Count: \" + str(len(df4_f)) + \"# below threshold: \" + str(len(df3_f[df4_f.Diff < threshold ])))\n",
    "plt.hist(df4_f[\"Diff\"])\n",
    "\n",
    "# df3_g = df2[df2.isGenuine==1][\"Diff\"]*10  #*0.0001\n",
    "# df4_g = [(d if d < threshold else threshold) for d in df3_g]#0.0000006]\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Genuine')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Total Count: \" + str(len(df3_g)) + \"# above threshold: \" + str(len(df3_g[df3_g.Diff > threshold ])))\n",
    "plt.hist(df3_g[\"Diff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/springboard/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diff</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Forgery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Diff</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.534244</td>\n",
       "      <td>0.534244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isGenuine</th>\n",
       "      <td>-0.534244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forgery</th>\n",
       "      <td>0.534244</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Diff  isGenuine   Forgery\n",
       "Diff       1.000000  -0.534244  0.534244\n",
       "isGenuine -0.534244   1.000000 -1.000000\n",
       "Forgery    0.534244  -1.000000  1.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"Forgery\"] =  [1 if d==0 else 0 for d in df2[\"isGenuine\"].values]\n",
    "df2.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9925\n",
    "\n",
    "df4_f[\"ToCompare\"] = df[df.isGenuine==0][\"ToCompare\"]\n",
    "df4_f[\"ToCompare\"] = df4_f[\"ToCompare\"].map(lambda x: x if  x > threshold else threshold-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_g = df[(df.isGenuine == 1)]\n",
    "df4_g = df3_g.copy(deep=True)\n",
    "df4_g[\"Pos\"] = df3_g[\"Pos\"].map(lambda x: x if  x > threshold else (threshold-0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Forgery')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Total Count: \" + str(len(df3_f)) + \"# below threshold: \" + str(len(df3_f[df4_f.ToCompare > threshold ])))\n",
    "plt.hist(df4_f[\"ToCompare\"])\n",
    "\n",
    "# df3_g = df2[df2.isGenuine==1][\"Diff\"]*10  #*0.0001\n",
    "# df4_g = [(d if d < threshold else threshold) for d in df3_g]#0.0000006]\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Genuine')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Total Count: \" + str(len(df3_g)) + \"# above threshold: \" + str(len(df3_g[df3_g.Pos < threshold ])))\n",
    "plt.hist(df4_g[\"Pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
