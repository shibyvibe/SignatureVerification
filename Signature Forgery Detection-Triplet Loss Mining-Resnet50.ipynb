{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Approach </b>\n",
    "\n",
    "1) Use Deep Neural networks - Generate embeddings for signatures genuine and forgery use a triplet loss mining approach to separate the features of forged signatures from genuine.\n",
    "2) Use Logistic regression for decisioning - use the embedding outputs from step1 and use these as features to train a logistics regression model to output the decision of genuine or forgery.\n",
    "   * A Deep Neural Network Model for decision would have worked better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"color: red;\">TODO </b>\n",
    "1) Add signatures from dataset1 to dataset2 mainly related to background color <br>\n",
    "2) Understand how to setup tensorflow GPU. Currently using Tensorflow CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tuning </b><p>\n",
    "To try\n",
    "* _compute_loss_sq change margin from 0.5 to 0.6   8/3 - Trying (need to retrain)\n",
    "* Try AutoKeras for hyper parameter tuning.\n",
    "* Try sklearn skitOptimizer\n",
    "* Try AutoGluon https://towardsdatascience.com/autogluon-deep-learning-automl-5cdb4e2388ec \n",
    "\n",
    "* Recalculate all the weights for ResNet50\n",
    "* Try MXNet is faster than tensorflow.\n",
    "* Add a custom layer which included the forgery decision to determine the weights (maybe use a MixMaxScaler to maximize the diff range)\n",
    "* Try a VGG16 with triplet loss mining.\n",
    "* Build a custom layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications  ##Tensor flow version used 2.4.1\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDimensions = (224,224,3)\n",
    "\n",
    "## Location to save the DNN model to generate Embeddings.\n",
    "filename = '/notebooks/capstone/models/embeddings-res32'        ##DNN Embeddings models save location\n",
    "filename2 = '/notebooks/capstone/models/siamesenetwork-res32'   ##DNN Siamesemodelsave location\n",
    "\n",
    "## Locations to pickle the generated embeddings for training data using trained model. \n",
    "#pFile_embeddings_train = \"/notebooks/capstone/train_embeddings.pickle\"  ##cosine\n",
    "pFile_embeddings_train = \"/notebooks/capstone/train_embeddings_k.pickle\"  ##kmean distance\n",
    "\n",
    "## Locations to pickle the generated embeddings for test data using trained model. \n",
    "pFile_embeddings_tst = \"/notebooks/capstone/tst_embeddings_k.pickle\"\n",
    "\n",
    "##LogisticRegression model saved.\n",
    "#pFile_lrmodel = \"/notebooks/capstone/train_lr_c.pickle\" #cosine\n",
    "pFile_lrmodel = \"/notebooks/capstone/train_lr_k.pickle\"  #kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Utility functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(filename):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPEG image, preprocess it and\n",
    "    resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_png(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, imageDimensions[:-1])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplets_train( anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "    print(\">>> Anchor\", anchor)\n",
    "\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplets(personId, anchor, positive, negative, isGenuine, anchor_path, positive_path, negative_path):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "    print(\">>> Anchor\", anchor)\n",
    "\n",
    "    return (\n",
    "        personId,\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "        isGenuine,\n",
    "        anchor_path, positive_path, negative_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(images_dataset):\n",
    "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
    "\n",
    "    def showImages(ax, image):\n",
    "        for i in range(3):\n",
    "            ax[i].imshow(image[i])\n",
    "            ax[i].get_xaxis().set_visible(False)\n",
    "            ax[i].get_yaxis().set_visible(False)\n",
    "\n",
    "    rows = len(images_dataset)\n",
    "    images = list(images_dataset.as_numpy_iterator())\n",
    "    fig, axs = plt.subplots(rows,3, sharex=True, sharey=True, figsize=(500,500))\n",
    "    for x in range(rows):\n",
    "         anchor, positive, negative = images[x][0],images[x][1],images[x][2]\n",
    "         showImages(axs[x], (anchor, positive, negative))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load training and test data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       company  personId person        fileName   relPath  Genuine  forged\n",
      "0    Acme Corp         1   Erin      001_01.PNG       001        1       0\n",
      "887  Acme Corp         1   Erin  0119001_01.png  001_forg        0       1\n",
      "894  Acme Corp         1   Erin  0201001_04.png  001_forg        0       1\n",
      "888  Acme Corp         1   Erin  0119001_02.png  001_forg        0       1\n",
      "889  Acme Corp         1   Erin  0119001_03.png  001_forg        0       1\n",
      "(1649, 7)\n"
     ]
    }
   ],
   "source": [
    "basePath = \"/notebooks/capstone/dataset/dataset2/sign_data\"\n",
    "data_train = pd.read_csv(basePath + \"/train/train_clean.csv\")\n",
    "data_train.sort_values(by=\"personId\", inplace=True)\n",
    "print(data_train.head())\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           company  personId person        fileName   relPath  Genuine  forged\n",
      "0    AI Novice Inc        49     IE      01_049.png       049        1       0\n",
      "255  AI Novice Inc        49     IE  02_0114049.PNG  049_forg        0       1\n",
      "256  AI Novice Inc        49     IE  02_0206049.PNG  049_forg        0       1\n",
      "257  AI Novice Inc        49     IE  02_0210049.PNG  049_forg        0       1\n",
      "252  AI Novice Inc        49     IE  01_0114049.PNG  049_forg        0       1\n",
      "258  AI Novice Inc        49     IE  03_0114049.PNG  049_forg        0       1\n",
      "259  AI Novice Inc        49     IE  03_0206049.PNG  049_forg        0       1\n",
      "260  AI Novice Inc        49     IE  03_0210049.PNG  049_forg        0       1\n",
      "261  AI Novice Inc        49     IE  04_0114049.PNG  049_forg        0       1\n",
      "262  AI Novice Inc        49     IE  04_0206049.PNG  049_forg        0       1\n",
      "263  AI Novice Inc        49     IE  04_0210049.PNG  049_forg        0       1\n",
      "254  AI Novice Inc        49     IE  01_0210049.PNG  049_forg        0       1\n",
      "253  AI Novice Inc        49     IE  01_0206049.PNG  049_forg        0       1\n",
      "9    AI Novice Inc        49     IE      10_049.png       049        1       0\n",
      "1    AI Novice Inc        49     IE      02_049.png       049        1       0\n",
      "2    AI Novice Inc        49     IE      03_049.png       049        1       0\n",
      "11   AI Novice Inc        49     IE      12_049.png       049        1       0\n",
      "3    AI Novice Inc        49     IE      04_049.png       049        1       0\n",
      "4    AI Novice Inc        49     IE      05_049.png       049        1       0\n",
      "5    AI Novice Inc        49     IE      06_049.png       049        1       0\n",
      "(500, 7)\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(basePath + \"/test/test_clean.csv\")\n",
    "data_test.sort_values(by=\"personId\", inplace=True)\n",
    "print(data_test.head(20))\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare training data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeImages(df, typeOfData):\n",
    "    personIds = df[\"personId\"].unique()\n",
    "    anchor_imgs = []\n",
    "    postive_imgs = []\n",
    "    negative_imgs = []\n",
    "    for p in personIds:\n",
    "        genuine = df[(df.personId==p) & (df.Genuine==1)]\n",
    "        forg = df[(df.personId==p) & (df.Genuine==0)]\n",
    "        anchor_img = basePath+\"/\"+typeOfData+\"/\"+genuine.iloc[0].relPath + \"/\"+genuine.iloc[0].fileName  \n",
    "        for g in genuine[1:].index:\n",
    "            pos_img= basePath+\"/\"+typeOfData+\"/\"+genuine.loc[g].relPath + \"/\"+genuine.loc[g].fileName  \n",
    "            for f in forg.index:\n",
    "                neg_img =  basePath+\"/\"+typeOfData+\"/\"+forg.loc[f].relPath + \"/\"+forg.loc[f].fileName  \n",
    "                anchor_imgs.append(anchor_img)\n",
    "                postive_imgs.append(pos_img)\n",
    "                negative_imgs.append(neg_img)\n",
    "    \n",
    "    return anchor_imgs,postive_imgs,negative_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images,positive_images,negative_images = categorizeImages(data_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Anchor Tensor(\"args_0:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "positive_dataset  = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "negative_dataset  = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "dataset = dataset.map(preprocess_triplets_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split our dataset in train and validation.\n",
    "image_count = len(anchor_dataset)\n",
    "train_dataset = dataset.take(round(image_count * 0.8))\n",
    "val_dataset = dataset.skip(round(image_count * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize(train_dataset.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define Evaluation Functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare each positive each positive in the set and and each negative against all other positives\n",
    "def categorizeTestImages(df, typeOfData):\n",
    "    personIds = df[\"personId\"].unique()\n",
    "    anchor_imgs = []\n",
    "    postive_imgs = []\n",
    "    toCompare_imgs = []\n",
    "    isGenuine = []\n",
    "    personId = []\n",
    "    for p in personIds:\n",
    "        genuine = df[(df.personId==p) & (df.Genuine==1)]\n",
    "        forg = df[(df.personId==p) & (df.Genuine==0)]\n",
    "        anchor_img = basePath+\"/\"+typeOfData+\"/\"+genuine.iloc[0].relPath + \"/\"+genuine.iloc[0].fileName  \n",
    "        for g in genuine[1:].index:\n",
    "            pos_img= basePath+\"/\"+typeOfData+\"/\"+genuine.loc[g].relPath + \"/\"+genuine.loc[g].fileName  \n",
    "            #Compare this with all forgeries\n",
    "            for f in forg.index:\n",
    "                toCompareImg =  basePath+\"/\"+typeOfData+\"/\"+forg.loc[f].relPath + \"/\"+forg.loc[f].fileName \n",
    "                personId.append(p)\n",
    "                anchor_imgs.append(anchor_img)\n",
    "                postive_imgs.append(pos_img)\n",
    "                toCompare_imgs.append(toCompareImg)\n",
    "                isGenuine.append(False)\n",
    "                \n",
    "            # Compare current postive with all other positives besides the anchor\n",
    "            for f in genuine[1:].index:\n",
    "                toCompareImg =  basePath+\"/\"+typeOfData+\"/\"+genuine.loc[f].relPath + \"/\"+genuine.loc[f].fileName  \n",
    "                if ( pos_img != toCompareImg):\n",
    "                    personId.append(p)\n",
    "                    anchor_imgs.append(anchor_img)\n",
    "                    postive_imgs.append(pos_img)\n",
    "                    toCompare_imgs.append(toCompareImg)\n",
    "                    isGenuine.append(True)    \n",
    "    return personId, anchor_imgs,postive_imgs,toCompare_imgs, isGenuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize(tst_dataset.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareEvaluationData(data_df, typeOfData):\n",
    "    personId, anchor_images,positive_images,toCompare_imgs, isGenuine = categorizeTestImages(data_df, typeOfData)\n",
    "    personId_dataset = tf.data.Dataset.from_tensor_slices(personId)\n",
    "    anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "    positive_dataset  = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "    toCompare_imgs_dataset  = tf.data.Dataset.from_tensor_slices(toCompare_imgs)\n",
    "    isGenuine_dataset  = tf.data.Dataset.from_tensor_slices(isGenuine)\n",
    "\n",
    "    anchor_path_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "    positive_path_dataset  = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "    toCompare_imgs_path_dataset  = tf.data.Dataset.from_tensor_slices(toCompare_imgs)\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((personId_dataset, anchor_dataset, positive_dataset, toCompare_imgs_dataset, isGenuine_dataset, anchor_path_dataset, positive_path_dataset, toCompare_imgs_path_dataset))\n",
    "    ##dataset = dataset.shuffle(buffer_size=1024)\n",
    "    dataset = dataset.map(preprocess_triplets)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Prepare model for Triplet Loss Mining </b> to compute the weights to generate embeddings for a signature specimen which will be closer to the genuine anchor signature and furthest from a forgery signature<br><br>\n",
    "<b>Prepare Model Architecture</b>\n",
    "Setting up the embedding generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=imageDimensions, include_top=False\n",
    ")\n",
    "\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "#     if layer.name == \"conv5_block1_out\":     ##TODO: Why only this layer?\n",
    "#         trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Embedding\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100352)       0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          51380736    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 75,168,640\n",
      "Trainable params: 51,579,392\n",
      "Non-trainable params: 23,589,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Setting up the Siamese Network model</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=imageDimensions)\n",
    "positive_input = layers.Input(name=\"positive\", shape=imageDimensions)\n",
    "negative_input = layers.Input(name=\"negative\", shape=imageDimensions)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(resnet.preprocess_input(anchor_input)),   ##TODO : What is pre-process input do here?\n",
    "    embedding(resnet.preprocess_input(positive_input)),\n",
    "    embedding(resnet.preprocess_input(negative_input)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances  ##TODO: Not clear what Output does here?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train Model - To Generate Embedding </b>\n",
    "<p>We now need to implement a model with custom training loop so we can compute the triplet loss using the three embeddings produced by the Siamese network.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "         \n",
    "    def _compute_loss_sq(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    \n",
    "#     def _compute_loss_cos(self, data):\n",
    "#         cosine_similarity = metrics.CosineSimilarity()\n",
    "#         sitive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "#         print(\"Positive similarity:\", positive_similarity)\n",
    "#         negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "#         print(\"Negative similarity\", negative_similarity)\n",
    "#         loss = tf.math.squared_difference(positive_similarity, negative_similarity)\n",
    "#         loss = tf.maximum(loss + self.margin, 0.0)\n",
    "#         #tf.math.squared_difference\n",
    "#         return loss\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        return self._compute_loss_sq(data)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "247/247 [==============================] - 231s 893ms/step - loss: 0.1086 - val_loss: 0.0589\n",
      "Epoch 2/40\n",
      "247/247 [==============================] - 218s 881ms/step - loss: 0.0879 - val_loss: 0.0532\n",
      "Epoch 3/40\n",
      "247/247 [==============================] - 217s 881ms/step - loss: 0.0686 - val_loss: 0.0486\n",
      "Epoch 4/40\n",
      "247/247 [==============================] - 218s 881ms/step - loss: 0.0571 - val_loss: 0.0498\n",
      "Epoch 5/40\n",
      "247/247 [==============================] - 218s 882ms/step - loss: 0.0498 - val_loss: 0.0437\n",
      "Epoch 6/40\n",
      "247/247 [==============================] - 216s 874ms/step - loss: 0.0429 - val_loss: 0.0410\n",
      "Epoch 7/40\n",
      "247/247 [==============================] - 215s 870ms/step - loss: 0.0357 - val_loss: 0.0448\n",
      "Epoch 8/40\n",
      "247/247 [==============================] - 215s 871ms/step - loss: 0.0313 - val_loss: 0.0401\n",
      "Epoch 9/40\n",
      "247/247 [==============================] - 214s 866ms/step - loss: 0.0275 - val_loss: 0.0336\n",
      "Epoch 10/40\n",
      "247/247 [==============================] - 215s 869ms/step - loss: 0.0256 - val_loss: 0.0370\n",
      "Epoch 11/40\n",
      "247/247 [==============================] - 215s 870ms/step - loss: 0.0227 - val_loss: 0.0352\n",
      "Epoch 12/40\n",
      "247/247 [==============================] - 214s 866ms/step - loss: 0.0208 - val_loss: 0.0287\n",
      "Epoch 13/40\n",
      "247/247 [==============================] - 214s 867ms/step - loss: 0.0192 - val_loss: 0.0315\n",
      "Epoch 14/40\n",
      "247/247 [==============================] - 213s 862ms/step - loss: 0.0160 - val_loss: 0.0305\n",
      "Epoch 15/40\n",
      "247/247 [==============================] - 212s 857ms/step - loss: 0.0141 - val_loss: 0.0276\n",
      "Epoch 16/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0137 - val_loss: 0.0297\n",
      "Epoch 17/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0119 - val_loss: 0.0234\n",
      "Epoch 18/40\n",
      "247/247 [==============================] - 211s 855ms/step - loss: 0.0104 - val_loss: 0.0252\n",
      "Epoch 19/40\n",
      "247/247 [==============================] - 211s 855ms/step - loss: 0.0095 - val_loss: 0.0227\n",
      "Epoch 20/40\n",
      "247/247 [==============================] - 211s 855ms/step - loss: 0.0084 - val_loss: 0.0219\n",
      "Epoch 21/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0081 - val_loss: 0.0213\n",
      "Epoch 22/40\n",
      "247/247 [==============================] - 212s 858ms/step - loss: 0.0071 - val_loss: 0.0220\n",
      "Epoch 23/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0069 - val_loss: 0.0221\n",
      "Epoch 24/40\n",
      "247/247 [==============================] - 211s 855ms/step - loss: 0.0057 - val_loss: 0.0205\n",
      "Epoch 25/40\n",
      "247/247 [==============================] - 211s 855ms/step - loss: 0.0056 - val_loss: 0.0198\n",
      "Epoch 26/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0047 - val_loss: 0.0195\n",
      "Epoch 27/40\n",
      "247/247 [==============================] - 212s 857ms/step - loss: 0.0038 - val_loss: 0.0198\n",
      "Epoch 28/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0036 - val_loss: 0.0179\n",
      "Epoch 29/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0035 - val_loss: 0.0190\n",
      "Epoch 30/40\n",
      "247/247 [==============================] - 211s 854ms/step - loss: 0.0030 - val_loss: 0.0192\n",
      "Epoch 31/40\n",
      "247/247 [==============================] - 211s 856ms/step - loss: 0.0030 - val_loss: 0.0191\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7dc062b0f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1)\n",
    "\n",
    "early_stop=[earlyStopping]\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0000001))\n",
    "siamese_model.fit(train_dataset, epochs=40, validation_data=val_dataset, callbacks=early_stop)  #8/8 - Changes from 20 to 40 to see if improve perf\n",
    "\n",
    "#Tuning\n",
    "# Image dimension=448, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=2.1\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=0.05\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, lossFn=cos, loss=? \n",
    "# Image dimension=224, margin=10 epoch 1, norestraining of weights,Adam=0.0001, loss=0.5\n",
    "# Image dimension=224, margin=0.5 epoch 1, norestraining of weights, Adam=0.00001 instead of 0.0001 loss=?**  0.000001 - 0.0020 - val_loss: 0.0095\n",
    "# Image dimension=112, margin=0.5 epoch 1, norestraining of weights,Adam=0.0001, loss=0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /notebooks/capstone/models/embeddings-res32/assets\n",
      "INFO:tensorflow:Assets written to: /notebooks/capstone/models/siamesenetwork-res32/assets\n"
     ]
    }
   ],
   "source": [
    "embedding.save(filename)\n",
    "siamese_network.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "siamese_network = tf.keras.models.load_model(filename2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate/Compare Embedding related utility functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(row):\n",
    "    personId, anchor, positive, toCcompare, isGenuine, anchor_path, positive_path, toCcompare_path = row\n",
    "    return (\n",
    "        personId.numpy(), \n",
    "        embedding(resnet.preprocess_input(anchor)),\n",
    "        embedding(resnet.preprocess_input(positive)),\n",
    "        embedding(resnet.preprocess_input(toCcompare)),\n",
    "        isGenuine.numpy()\n",
    "        ,anchor_path.numpy()\n",
    "        ,positive_path.numpy()\n",
    "        ,toCcompare_path.numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingDataFrame(data_df, typeOfData):\n",
    "    dataset = prepareEvaluationData(data_df, typeOfData)\n",
    "    dataset = dataset.batch(1, drop_remainder=False)\n",
    "    embedding_data = [getEmbeddings(row) for row in iter(dataset)]\n",
    "    embeddings_data_df = pd.DataFrame(columns=[\"personId\", \"anchor_embedding\", \"positive_embedding\",\"negative_embedding\",\"isGenuine\", \"Anchor_Path\", \"Pos_Path\", \"ToComparePath\"], data=embedding_data)\n",
    "    return embeddings_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareEmbeds_row_cosine (personId, anchor_embedding, positive_embedding, negative_embedding, isGenuine):\n",
    "    cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "    positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "    #print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "    negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "    #print(\"Negative similarity\", negative_similarity.numpy())\n",
    "    return (positive_similarity.numpy(), negative_similarity.numpy(), (positive_similarity.numpy() - negative_similarity.numpy()))\n",
    "\n",
    "def compareEmbeds_row_kMeansdistance (personId, anchor_embedding, positive_embedding, negative_embedding, isGenuine):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor_embedding - positive_embedding), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor_embedding - negative_embedding), -1)\n",
    "        return (ap_distance, an_distance, (ap_distance-an_distance))\n",
    "\n",
    "def compareEmbeds(df):\n",
    "    df2 = df.apply(lambda row: compareEmbeds_row_kMeansdistance(row.personId, row.anchor_embedding, row.positive_embedding, row.negative_embedding, row.isGenuine), axis=1,  result_type='expand')\n",
    "    df2.columns = [\"Pos\", \"Neg\", \"Diff\"]\n",
    "\n",
    "    df[[\"Pos\", \"Neg\", \"Diff\"]]=df2[[\"Pos\", \"Neg\", \"Diff\"]]\n",
    "    df[\"personId\"]=df[\"personId\"].apply(lambda x: x[0])\n",
    "    df[\"isGenuine\"]=df[\"isGenuine\"].apply(lambda x: x[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train Decision Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "##Load model\n",
    "embedding = tf.keras.models.load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Anchor Tensor(\"args_1:0\", shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Anchor_Path</th>\n",
       "      <th>Pos_Path</th>\n",
       "      <th>ToComparePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1]</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.3195522, shape=(), dtype=float32...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  personId                                   anchor_embedding  \\\n",
       "0      [1]  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "\n",
       "                                  negative_embedding isGenuine  \\\n",
       "0  ((tf.Tensor(0.3195522, shape=(), dtype=float32...   [False]   \n",
       "\n",
       "                                         Anchor_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                            Pos_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                       ToComparePath  \n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_data_df_train = getEmbeddingDataFrame(data_train, \"train\")\n",
    "embeddings_data_df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pFile_embeddings_train, 'wb') as file:\n",
    "    pickle.dump(embeddings_data_df_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20798, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pFile_embeddings_train, 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Anchor_Path</th>\n",
       "      <th>Pos_Path</th>\n",
       "      <th>ToComparePath</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.3195522, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(3.466082, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-1.3339517, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.4272677, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(6.6205983, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-4.488468, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.17955683, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(3.6923869, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-1.5602565, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "1         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "2         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "1  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "2  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "\n",
       "                                  negative_embedding  isGenuine  \\\n",
       "0  ((tf.Tensor(0.3195522, shape=(), dtype=float32...      False   \n",
       "1  ((tf.Tensor(0.4272677, shape=(), dtype=float32...      False   \n",
       "2  ((tf.Tensor(0.17955683, shape=(), dtype=float3...      False   \n",
       "\n",
       "                                         Anchor_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                            Pos_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                       ToComparePath  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                               Pos  \\\n",
       "0  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "\n",
       "                                               Neg  \\\n",
       "0   (tf.Tensor(3.466082, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(6.6205983, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(3.6923869, shape=(), dtype=float32))   \n",
       "\n",
       "                                               Diff  \n",
       "0  (tf.Tensor(-1.3339517, shape=(), dtype=float32))  \n",
       "1   (tf.Tensor(-4.488468, shape=(), dtype=float32))  \n",
       "2  (tf.Tensor(-1.5602565, shape=(), dtype=float32))  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare embedding with the anchor\n",
    "df = compareEmbeds(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Anchor_Path</th>\n",
       "      <th>Pos_Path</th>\n",
       "      <th>ToComparePath</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "      <th>isForgery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.3195522, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(3.466082, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-1.3339517, shape=(), dtype=float32))</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.4272677, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(6.6205983, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-4.488468, shape=(), dtype=float32))</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>((tf.Tensor(0.19811925, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.22750023, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.17955683, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(2.1321304, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(3.6923869, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-1.5602565, shape=(), dtype=float32))</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "1         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "2         1  ((tf.Tensor(0.19811925, shape=(), dtype=float3...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "1  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "2  ((tf.Tensor(0.22750023, shape=(), dtype=float3...   \n",
       "\n",
       "                                  negative_embedding  isGenuine  \\\n",
       "0  ((tf.Tensor(0.3195522, shape=(), dtype=float32...      False   \n",
       "1  ((tf.Tensor(0.4272677, shape=(), dtype=float32...      False   \n",
       "2  ((tf.Tensor(0.17955683, shape=(), dtype=float3...      False   \n",
       "\n",
       "                                         Anchor_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                            Pos_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                       ToComparePath  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                               Pos  \\\n",
       "0  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(2.1321304, shape=(), dtype=float32))   \n",
       "\n",
       "                                               Neg  \\\n",
       "0   (tf.Tensor(3.466082, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(6.6205983, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(3.6923869, shape=(), dtype=float32))   \n",
       "\n",
       "                                               Diff  isForgery  \n",
       "0  (tf.Tensor(-1.3339517, shape=(), dtype=float32))       True  \n",
       "1   (tf.Tensor(-4.488468, shape=(), dtype=float32))       True  \n",
       "2  (tf.Tensor(-1.5602565, shape=(), dtype=float32))       True  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_forgery = df.assign(isForgery=lambda x: (0 == x.isGenuine))\n",
    "df_train_forgery.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a logistic regression model for predictions\n",
    "#lr = LogisticRegression(random_state=0).fit(df[[\"personId\", \"Pos\", \"Neg\"]], df[\"isGenuine\"])\n",
    "lr = LogisticRegression(random_state=0).fit(df_train_forgery[[\"personId\", \"Pos\", \"Neg\"]], df_forgery[\"isForgery\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pFile_lrmodel, 'wb') as file:\n",
    "    pickle.dump(lr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pFile_lrmodel, 'rb') as file:\n",
    "    lr = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = lr.predict(df_train_forgery[[\"personId\", \"Pos\", \"Neg\"]])  #neg = tocompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921605"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fpr, tpr, thresholds = metrics.roc_curve(df[\"isGenuine\"], y_train_predict, pos_label=2)\n",
    "# metrics.auc(fpr, tpr)\n",
    "m_train = metrics.AUC()\n",
    "m_train.update_state(df_train_forgery[\"isForgery\"], y_train_predict)\n",
    "m_train.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[10318,   638],\n",
       "       [  970,  8872]], dtype=int32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics.confusion_matrix(df[\"isGenuine\"], y_train_predict)\n",
    "tf.math.confusion_matrix(df_train_forgery[\"isForgery\"], y_train_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Testing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting what the network has learned\n",
    "At this point, we can check how the network learned to separate the embeddings depending on whether they belong to similar images.\n",
    "\n",
    "We can use cosine similarity to measure the similarity between embeddings.\n",
    "\n",
    "Let's pick a sample from the dataset to check the similarity between the embeddings generated for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the cosine similarity between the anchor and positive images and compare it with the similarity between the anchor and the negative images.\n",
    "\n",
    "We should expect the similarity between the anchor and positive images to be larger than the similarity between the anchor and the negative images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "##Load model\n",
    "embedding = tf.keras.models.load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Anchor Tensor(\"args_1:0\", shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Anchor_Path</th>\n",
       "      <th>Pos_Path</th>\n",
       "      <th>ToComparePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[49]</td>\n",
       "      <td>((tf.Tensor(0.1990531, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.18121363, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.10500425, shape=(), dtype=float3...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  personId                                   anchor_embedding  \\\n",
       "0     [49]  ((tf.Tensor(0.1990531, shape=(), dtype=float32...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.18121363, shape=(), dtype=float3...   \n",
       "\n",
       "                                  negative_embedding isGenuine  \\\n",
       "0  ((tf.Tensor(0.10500425, shape=(), dtype=float3...   [False]   \n",
       "\n",
       "                                         Anchor_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                            Pos_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                       ToComparePath  \n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_data_df_tst = getEmbeddingDataFrame(data_test, \"test\")\n",
    "embeddings_data_df_tst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pFile_embeddings_tst, 'wb') as file:\n",
    "    pickle.dump(embeddings_data_df_tst, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5038, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pFile_embeddings_tst, 'rb') as file:\n",
    "    df_tst = pickle.load(file)\n",
    "df_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>anchor_embedding</th>\n",
       "      <th>positive_embedding</th>\n",
       "      <th>negative_embedding</th>\n",
       "      <th>isGenuine</th>\n",
       "      <th>Anchor_Path</th>\n",
       "      <th>Pos_Path</th>\n",
       "      <th>ToComparePath</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.1990531, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.18121363, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.10500425, shape=(), dtype=float3...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(1.3259882, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(8.532532, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-7.2065434, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.1990531, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.18121363, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.2641765, shape=(), dtype=float32...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(1.3259882, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(1.699486, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-0.37349784, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>((tf.Tensor(0.1990531, shape=(), dtype=float32...</td>\n",
       "      <td>((tf.Tensor(0.18121363, shape=(), dtype=float3...</td>\n",
       "      <td>((tf.Tensor(0.050875347, shape=(), dtype=float...</td>\n",
       "      <td>False</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>[b'/notebooks/capstone/dataset/dataset2/sign_d...</td>\n",
       "      <td>(tf.Tensor(1.3259882, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(5.999592, shape=(), dtype=float32))</td>\n",
       "      <td>(tf.Tensor(-4.6736035, shape=(), dtype=float32))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                   anchor_embedding  \\\n",
       "0        49  ((tf.Tensor(0.1990531, shape=(), dtype=float32...   \n",
       "1        49  ((tf.Tensor(0.1990531, shape=(), dtype=float32...   \n",
       "2        49  ((tf.Tensor(0.1990531, shape=(), dtype=float32...   \n",
       "\n",
       "                                  positive_embedding  \\\n",
       "0  ((tf.Tensor(0.18121363, shape=(), dtype=float3...   \n",
       "1  ((tf.Tensor(0.18121363, shape=(), dtype=float3...   \n",
       "2  ((tf.Tensor(0.18121363, shape=(), dtype=float3...   \n",
       "\n",
       "                                  negative_embedding  isGenuine  \\\n",
       "0  ((tf.Tensor(0.10500425, shape=(), dtype=float3...      False   \n",
       "1  ((tf.Tensor(0.2641765, shape=(), dtype=float32...      False   \n",
       "2  ((tf.Tensor(0.050875347, shape=(), dtype=float...      False   \n",
       "\n",
       "                                         Anchor_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                            Pos_Path  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                       ToComparePath  \\\n",
       "0  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "1  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "2  [b'/notebooks/capstone/dataset/dataset2/sign_d...   \n",
       "\n",
       "                                               Pos  \\\n",
       "0  (tf.Tensor(1.3259882, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(1.3259882, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(1.3259882, shape=(), dtype=float32))   \n",
       "\n",
       "                                              Neg  \\\n",
       "0  (tf.Tensor(8.532532, shape=(), dtype=float32))   \n",
       "1  (tf.Tensor(1.699486, shape=(), dtype=float32))   \n",
       "2  (tf.Tensor(5.999592, shape=(), dtype=float32))   \n",
       "\n",
       "                                                Diff  \n",
       "0   (tf.Tensor(-7.2065434, shape=(), dtype=float32))  \n",
       "1  (tf.Tensor(-0.37349784, shape=(), dtype=float32))  \n",
       "2   (tf.Tensor(-4.6736035, shape=(), dtype=float32))  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst = compareEmbeds(df_tst)\n",
    "df_tst.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_forgery = df_tst.assign(isForgery=lambda x: (0 == x.isGenuine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tst_predict = lr.predict(df_test_forgery[[\"personId\", \"Pos\", \"Neg\"]])  #neg = tocompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88121074"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn import metrics\n",
    "# fpr_tst, tpr_tst, thresholds_tst = metrics.roc_curve(df_tst[\"isGenuine\"], y_tst_predict, pos_label=2)\n",
    "# metrics.auc(fpr_tst, tpr_tst)\n",
    "m_tst = metrics.AUC()\n",
    "m_tst.update_state(df_test_forgery[\"isGenuine\"], y_tst_predict)\n",
    "m_tst.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2127,  183],\n",
       "       [ 432, 2296]], dtype=int32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics.confusion_matrix(df_tst[\"isGenuine\"], y_tst_predict)\n",
    "tf.math.confusion_matrix(df_test_forgery[\"isForgery\"], y_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8416422"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_tst_recall = metrics.Recall()\n",
    "m_tst_recall.update_state(df_test_forgery[\"isForgery\"], y_tst_predict)\n",
    "m_tst_recall.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261799"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_tst_precision = metrics.Precision()\n",
    "m_tst_precision.update_state(df_test_forgery[\"isForgery\"], y_tst_predict)\n",
    "m_tst_precision.result().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
